{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Mock Technical Interviewer with Gemini & LangGraph\n\n**Google GenAI Capstone Project**\n\n**Author:** Maksym Rimashevskyi\n\n## 📖 1. Introduction: Bridging the Technical Interview Preparation Gap with GenAI\n\n### The Problem: The High Stakes and Limitations of Traditional Technical Interview Practice 😩\nPreparing for technical interviews in the competitive tech industry is a critical yet often inefficient and stressful process. Candidates frequently encounter limitations such as:\n\n*   **😬 Lack of Realistic Simulation:** Existing resources often fail to replicate the dynamic, conversational nature and pressure of a real technical interview.\n*   **🤦‍♀️ Subjective and Delayed Feedback:** Feedback from human mock interviews can be subjective, inconsistent, and often comes with delays, hindering rapid learning.\n*   **💸 Limited Accessibility and Scalability:** Access to experienced human interviewers for practice is often restricted by cost and availability.\n*   **🧭 Difficulty in Identifying Specific Weaknesses:** Candidates may struggle to pinpoint precise areas for improvement beyond general feedback.\n\n### The Solution: An Intelligent GenAI-Powered Mock Interview Platform 🧠\n\nThis project introduces an innovative solution: an intelligent, AI-driven Mock Technical Interview Platform designed to provide realistic, interactive, and personalized interview practice. By harnessing the advanced capabilities of Google's Gemini models and the LangGraph framework, this platform offers a transformative approach to technical interview preparation.\n\n**Key Features:**\n*   **🗣️ Dynamic Interview Simulation (LangGraph Agents):** Employs LangGraph agents powered by Gemini to create a stateful, multi-turn conversational experience that mirrors a real technical interview, guiding candidates through problem-solving.\n*   **💻 Interactive Problem Engagement (Gemini):** Allows candidates to engage with coding problems through text, code input, and even visual explanations via whiteboard sketches interpreted by Gemini's image understanding capabilities.\n*   **💡 Intelligent Guidance and Probing (Few-Shot Learning):** Leverages few-shot learning within prompts to enable Gemini to provide contextually relevant hints, ask clarifying questions, and adapt its approach based on the candidate's responses, mimicking the nuanced interaction of a human interviewer.\n*   **❓ Knowledge Retrieval for Questioning (Function Calling):** Utilizes function calling to dynamically retrieve relevant interview questions from a database, ensuring a diverse and potentially tailored interview experience.\n*   **📊 Comprehensive and Structured Performance Evaluation (Structured Output):** Generates detailed, structured reports using Gemini's controlled generation capabilities, objectively assessing technical proficiency, problem-solving methodology, communication clarity, and code quality.\n*   **📚 Personalized Learning and Skill Enhancement (Grounding):** Employs Gemini's grounding capabilities (integrating with external knowledge sources) to analyze the interview performance and generate a targeted learning plan with relevant resources, directly addressing identified weaknesses.\n*   **🧠 Contextual Awareness and Long-Term Analysis (Long Context Window):** Leverages Gemini's long context window to maintain a comprehensive understanding of the entire interview conversation, enabling nuanced feedback and holistic performance analysis.\n\n### Innovation & Use Case Suitability ✨\nThis project represents a significant step forward in technical interview preparation through its creative and impactful application of Generative AI.\n*   **🌟 Novelty:** It goes beyond simple Q&A by constructing a sophisticated, stateful interviewing agent capable of understanding multimodal input, providing adaptive guidance, and generating nuanced evaluations – a level of interaction previously only achievable with human interviewers. The integration of whiteboard understanding directly into a technical interview simulation is particularly novel.\n*   **🚀 Impact:** This platform has the potential to democratize access to high-quality interview practice, enabling a wider range of candidates to prepare effectively, reduce interview anxiety, and ultimately improve their chances of success in the competitive tech job market. Its scalability allows for widespread adoption and consistent, objective feedback.\n*   **💯 GenAI Suitability:** The project's core functionalities are exceptionally well-suited to the strengths of modern GenAI models like Gemini:\n    *   **🗣️ Natural Language Understanding and Generation:** Crucial for engaging in a realistic dialogue, interpreting candidate responses, and formulating insightful questions and feedback.\n    *   **💻 Code Understanding and Generation:** Essential for interacting with code snippets provided by the candidate and even generating follow-up questions based on the code.\n    *   **🖼️ Image Understanding:** Enables the innovative feature of interpreting whiteboard sketches, a common element in real technical interviews.\n    *   **📑 Structured Output and Function Calling:** Allows for the systematic retrieval of questions and the generation of organized and informative performance reports.\n    *   **🤖 Agentic Capabilities (LangGraph):** Necessary for orchestrating the complex, multi-step process of a technical interview, managing state, and directing the flow of the conversation.\n    *   **⏳ Long Context Window:** Enables the AI to maintain context throughout the interview for more accurate and relevant feedback.\n    *   **🌍 Grounding:** Facilitates the generation of personalized and actionable learning plans based on external knowledge.\n\nBy seamlessly integrating these advanced GenAI capabilities, this project offers a powerful and innovative solution to the persistent challenges of technical interview preparation.","metadata":{}},{"cell_type":"markdown","source":"### ❗ **IMPORTANT!**\n\nThe app built in this notebook uses Gradio launch function to run an UI interface. This section is commented-out to ensure that you can use the `Run all` feature without interruption. At the end of this notebook you will need to uncomment the `demo.launch(...)` call in order to interact with the app.\n\nIf you wish to save a version of this notebook with `Save and Run all`, you will need to **re-comment** the line you commented out to ensure that the notebook can run without human input.","metadata":{}},{"cell_type":"markdown","source":"## ⚙️ 2. Environment Setup\n\nThis section handles the necessary setup for the project environment.\n\n1.  **Package Installation:** We first uninstall potentially conflicting default Kaggle packages and then install the required libraries:\n    *   `gradio`: For building the interactive web UI.\n    *   `langgraph`: The core framework for building the stateful agent.\n    *   `langchain-google-genai`: Provides LangChain integrations for Gemini models.\n    *   `langgraph-prebuilt`: Contains pre-built LangGraph components (though we build custom ones).\n    *   `google-genai`: Provides an interface to integrate Google's generative models.\n    *   Other dependencies implicitly installed include `langchain-core`, `pydantic`, etc.\n2.  **Library Imports:** We import standard Python libraries, third-party tools (like `gradio`, `pandas`, `PIL`), and specific components from `langchain`, `langgraph`, and `google.genai`.\n3.  **API Key Configuration:** We securely load the `GOOGLE_API_KEY` using Kaggle Secrets and set it as an environment variable for the Gemini client.","metadata":{}},{"cell_type":"markdown","source":"### 2.1 Install packages","metadata":{}},{"cell_type":"markdown","source":"Remove conflicting packages from the Kaggle base environment and install langgraph and google-genai packages.","metadata":{}},{"cell_type":"code","source":"!pip uninstall -qqy jupyterlab libpysal thinc spacy fastai ydata-profiling google-cloud-bigquery google-generativeai pydantic\n\n!pip install -qqU gradio langgraph==0.3.21 langchain-google-genai==2.1.2 langgraph-prebuilt==0.1.7 google-genai==1.7.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T20:57:38.122845Z","iopub.execute_input":"2025-04-17T20:57:38.123102Z","iopub.status.idle":"2025-04-17T20:58:14.419580Z","shell.execute_reply.started":"2025-04-17T20:57:38.123081Z","shell.execute_reply":"2025-04-17T20:58:14.418659Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.2/65.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.9/433.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.6/443.6 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\njupyterlab-lsp 3.10.2 requires jupyterlab<4.0.0a0,>=3.1.0, which is not installed.\ngoogle-cloud-aiplatform 1.79.0 requires google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0, which is not installed.\nsigstore 3.6.1 requires rich~=13.0, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"### 2.2 Import libraries","metadata":{}},{"cell_type":"code","source":"# Standard Library\nimport base64\nimport json\nimport os\nfrom io import BytesIO\nfrom typing import Annotated, List, Literal, Optional, TypedDict, Union, Dict, Any, Tuple\n\n# Third-Party\nimport gradio as gr\nimport numpy as np\nimport pandas as pd\nimport PIL.Image\nimport requests\nfrom google import genai\nfrom google.genai import types\nfrom IPython.display import Image, Markdown, display\nfrom jinja2 import Template\nfrom kaggle_secrets import UserSecretsClient\nfrom langchain_core.messages import BaseMessage\nfrom langchain_core.messages.ai import AIMessage\nfrom langchain_core.messages.human import HumanMessage\nfrom langchain_core.messages.system import SystemMessage\nfrom langchain_core.messages.tool import ToolMessage\nfrom langchain_core.runnables import Runnable\nfrom langchain_core.tools import BaseTool, tool\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom langgraph.graph import END, START, StateGraph \nfrom langgraph.graph.message import add_messages\nfrom langgraph.prebuilt import ToolNode\nfrom pydantic import BaseModel, Field, ConfigDict\n\ngenai.__version__","metadata":{"execution":{"iopub.status.busy":"2025-04-17T20:58:14.421500Z","iopub.execute_input":"2025-04-17T20:58:14.422178Z","iopub.status.idle":"2025-04-17T20:58:19.950588Z","shell.execute_reply.started":"2025-04-17T20:58:14.422150Z","shell.execute_reply":"2025-04-17T20:58:19.949813Z"},"trusted":true},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'1.7.0'"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"### 2.3 Loading secrets","metadata":{}},{"cell_type":"markdown","source":"To run the following cell, your API key must be stored it in a [Kaggle secret](https://www.kaggle.com/discussions/product-feedback/114053) named `GOOGLE_API_KEY`.\n \nIf you don't already have an API key, you can grab one from [AI Studio](https://aistudio.google.com/app/apikey). You can find [detailed instructions in the docs](https://ai.google.dev/gemini-api/docs/api-key).\n \nTo make the key available through Kaggle secrets, choose `Secrets` from the `Add-ons` menu and follow the instructions to add your key or enable it for this notebook.","metadata":{}},{"cell_type":"code","source":"GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\nos.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T20:58:19.951330Z","iopub.execute_input":"2025-04-17T20:58:19.952163Z","iopub.status.idle":"2025-04-17T20:58:20.249699Z","shell.execute_reply.started":"2025-04-17T20:58:19.952141Z","shell.execute_reply":"2025-04-17T20:58:20.248313Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"If you received an error response along the lines of `No user secrets exist for kernel id ...`, then you need to add your API key via `Add-ons`, `Secrets` **and** enable it.\n \n![Screenshot of the checkbox to enable GOOGLE_API_KEY secret](https://storage.googleapis.com/kaggle-media/Images/5gdai_sc_3.png)","metadata":{}},{"cell_type":"markdown","source":"**Automated retry**\n\nSet up an automatic retry that ensures your requests are retried when per-minute quota is reached.","metadata":{}},{"cell_type":"code","source":"from google.api_core import retry\n\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\nif not hasattr(genai.models.Models.generate_content, '__wrapped__'):\n  genai.models.Models.generate_content = retry.Retry(\n      predicate=is_retriable)(genai.models.Models.generate_content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T20:58:20.251832Z","iopub.execute_input":"2025-04-17T20:58:20.252127Z","iopub.status.idle":"2025-04-17T20:58:20.257804Z","shell.execute_reply.started":"2025-04-17T20:58:20.252098Z","shell.execute_reply":"2025-04-17T20:58:20.256601Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## 🧩 3. Data Preparation: Interview Questions\n\nTo provide relevant coding challenges, the interviewer needs access to a database of problems.\n\n*   **Data Source:** We download a `data.json` file containing a list of coding problems. This file is a simulated database of questions captured on [LeetCode](leetcode.com) that includes fields like `id`, `problem_name`, `topic`, `difficulty`, `link`, `companies`, `content` (problem description), and `code` (starter code). Data has JSON format and stored using GitHub gists.\n*   **Loading Data:** The JSON data is loaded into a Pandas DataFrame (`df`) for easy filtering and sampling within the application logic. This DataFrame serves as the knowledge base for the interview questions.","metadata":{}},{"cell_type":"code","source":"!wget -O data.json https://gist.githubusercontent.com/MaxDatex/474441cecf63e4a71138029f5d9ec1d6/raw/a43227f3f671fb09b08285e8097756ad6e050d2d/data.json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T20:58:20.258716Z","iopub.execute_input":"2025-04-17T20:58:20.259010Z","iopub.status.idle":"2025-04-17T20:58:21.131902Z","shell.execute_reply.started":"2025-04-17T20:58:20.258985Z","shell.execute_reply":"2025-04-17T20:58:21.130610Z"}},"outputs":[{"name":"stdout","text":"--2025-04-17 20:58:20--  https://gist.githubusercontent.com/MaxDatex/474441cecf63e4a71138029f5d9ec1d6/raw/a43227f3f671fb09b08285e8097756ad6e050d2d/data.json\nResolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\nConnecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 20937 (20K) [text/plain]\nSaving to: ‘data.json’\n\ndata.json           100%[===================>]  20.45K  --.-KB/s    in 0.007s  \n\n2025-04-17 20:58:21 (2.72 MB/s) - ‘data.json’ saved [20937/20937]\n\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"file_path = \"data.json\"\n\ntry:\n    with open(file_path, 'r') as f:\n        problems_data = json.load(f)\n    print(\"JSON data loaded successfully!\")\nexcept FileNotFoundError:\n    print(f\"Error: File not found at {file_path}\")\nexcept json.JSONDecodeError:\n    print(f\"Error: Could not decode JSON from {file_path}. Check the file content.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T20:58:21.132919Z","iopub.execute_input":"2025-04-17T20:58:21.133151Z","iopub.status.idle":"2025-04-17T20:58:21.139632Z","shell.execute_reply.started":"2025-04-17T20:58:21.133131Z","shell.execute_reply":"2025-04-17T20:58:21.138829Z"}},"outputs":[{"name":"stdout","text":"JSON data loaded successfully!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"df = pd.read_json(file_path)\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T20:58:21.140758Z","iopub.execute_input":"2025-04-17T20:58:21.141059Z","iopub.status.idle":"2025-04-17T20:58:21.199656Z","shell.execute_reply.started":"2025-04-17T20:58:21.141034Z","shell.execute_reply":"2025-04-17T20:58:21.198673Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"   id                topic                                               link  \\\n0   0   Array Manipulation              https://leetcode.com/problems/two-sum   \n1   1   Array Manipulation  https://leetcode.com/problems/container-with-m...   \n2   2   Array Manipulation                 https://leetcode.com/problems/3sum   \n3   3  String Manipulation  https://leetcode.com/problems/longest-substrin...   \n4   4  String Manipulation  https://leetcode.com/problems/string-to-intege...   \n\n                                     problem_name difficulty  \\\n0                                         Two sum       Easy   \n1                       Container with most water     Medium   \n2                                            3sum     Medium   \n3  Longest substring without repeating characters     Medium   \n4                        String to integer (atoi)     Medium   \n\n                                        companies  \\\n0  [OpenAI, Google, Amazon, Meta, Netflix, Apple]   \n1                                [Google, Amazon]   \n2  [OpenAI, Google, Amazon, Meta, Netflix, Apple]   \n3  [OpenAI, Google, Amazon, Meta, Netflix, Apple]   \n4                                  [Meta, Amazon]   \n\n                                             content  \\\n0  Given an array of integers nums and an integer...   \n1  'You are given an integer array height of leng...   \n2  Given an integer array nums, return all the tr...   \n3  Given a string s, find the length of the longe...   \n4  Implement the myAtoi(string s) function, which...   \n\n                                                code  \n0  class Solution:\\n    def twoSum(self, nums: Li...  \n1  class Solution:\\n    def maxArea(self, height:...  \n2  class Solution:\\n    def threeSum(self, nums: ...  \n3  class Solution:\\n    def lengthOfLongestSubstr...  \n4  class Solution:\\n    def myAtoi(self, s: str) ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>topic</th>\n      <th>link</th>\n      <th>problem_name</th>\n      <th>difficulty</th>\n      <th>companies</th>\n      <th>content</th>\n      <th>code</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Array Manipulation</td>\n      <td>https://leetcode.com/problems/two-sum</td>\n      <td>Two sum</td>\n      <td>Easy</td>\n      <td>[OpenAI, Google, Amazon, Meta, Netflix, Apple]</td>\n      <td>Given an array of integers nums and an integer...</td>\n      <td>class Solution:\\n    def twoSum(self, nums: Li...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Array Manipulation</td>\n      <td>https://leetcode.com/problems/container-with-m...</td>\n      <td>Container with most water</td>\n      <td>Medium</td>\n      <td>[Google, Amazon]</td>\n      <td>'You are given an integer array height of leng...</td>\n      <td>class Solution:\\n    def maxArea(self, height:...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Array Manipulation</td>\n      <td>https://leetcode.com/problems/3sum</td>\n      <td>3sum</td>\n      <td>Medium</td>\n      <td>[OpenAI, Google, Amazon, Meta, Netflix, Apple]</td>\n      <td>Given an integer array nums, return all the tr...</td>\n      <td>class Solution:\\n    def threeSum(self, nums: ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>String Manipulation</td>\n      <td>https://leetcode.com/problems/longest-substrin...</td>\n      <td>Longest substring without repeating characters</td>\n      <td>Medium</td>\n      <td>[OpenAI, Google, Amazon, Meta, Netflix, Apple]</td>\n      <td>Given a string s, find the length of the longe...</td>\n      <td>class Solution:\\n    def lengthOfLongestSubstr...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>String Manipulation</td>\n      <td>https://leetcode.com/problems/string-to-intege...</td>\n      <td>String to integer (atoi)</td>\n      <td>Medium</td>\n      <td>[Meta, Amazon]</td>\n      <td>Implement the myAtoi(string s) function, which...</td>\n      <td>class Solution:\\n    def myAtoi(self, s: str) ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"## 🧠 4. Core Logic: The Technical Interviewer Agent (LangGraph)\n\nThe heart of this application is the agent built using LangGraph, designed to simulate the flow and logic of a technical interview. LangGraph allows us to define a state machine or graph where nodes represent processing steps (like invoking the LLM or tools) and edges represent the flow between these steps.\n\n### 4.1. Agent Prompts 🔑➡️🤖\n\nPrompts are crucial for guiding the Gemini model's behavior. We define several key prompts:\n\n*   **`INTERVIEWER_SYSTEM_PROMPT`:** This is the main prompt defining the AI's persona and core responsibilities.\n    *   **Persona:** A supportive technical interviewer from \"Mock Technologie Inc.\"\n    *   **Goal:** Evaluate the candidate's skills while guiding them through a problem.\n    *   **Behavior:** Ask probing questions, provide *graduated hints* (Level 1-3), encourage \"thinking out loud,\" confirm understanding.\n    *   **Constraints:** Must use tools to get questions, cannot make up questions, needs confirmation for starting/ending the interview.\n    *   **Multi-Modal Handling:** Explicit instructions on how to analyze and integrate information from whiteboard screenshots seamlessly into the conversation.\n    *   **Few-Shot Examples:** Includes example interactions to demonstrate desired behavior, especially regarding hint provision and whiteboard analysis.\n*   **`WELCOME_MSG`:** The initial message the interviewer presents to the user.\n*   **`CANDIDATE_EVALUATION_PROMPT`:** Used *after* the interview ends. It instructs a separate LLM call to analyze the full transcript and final code, evaluating the candidate based on specific criteria (Technical Competence, Problem-Solving, Communication). Crucially, it requests the output in a **structured JSON format**.\n*   **`RESOURCES_SEARCH_PROMPT`:** Used to generate the personalized learning plan. It provides context (interview question, evaluation summary, topics to learn) and instructs the LLM to use the **Google Search tool (Grounding)** to find relevant resources and synthesize recommendations, *without* explicitly listing URLs in the text (citations are handled automatically).\n*   **`DESCRIBE_IMAGE_PROMPT`:** A specialized prompt used by the `get_interview_transcript` utility function. When a whiteboard image is present in the user's message, this prompt asks Gemini to describe the image's content and relevance to the ongoing conversation, making the transcript more comprehensive for the final evaluation.","metadata":{}},{"cell_type":"markdown","source":"**Few words about the following prompt**\n\nThe first iteration of this prompt was created using materials from the Internet about how to be a good technical interviewer.\nThen, step by step, I improved this prompt to make the model do what I wanted it to and behave more like a real technical interviewer.\nHere are a few things that needed to be made to make the model work properly:\n1. Add a line telling the model to ask the user for confirmation (confirmation used to invoke tools)\n2. Accent on the fact, that the model should only take questions from the database (using tools).\n3. While testing, the model could give some strange responses. Using a few examples (few-shot learning) helps mitigate these issues.  \n\nNevertheless, I think there could be a lot more things to change/add to this prompt, but we need to test it more.","metadata":{}},{"cell_type":"code","source":"INTERVIEWER_SYSTEM_PROMPT = '''\n\nCOMPANY NAME: \"Mock Technologie Inc.\"\nYou are a technical interviewer and an expert in software engineering, technical interviewing, and pedagogical best practices. \nYour primary goal is to evaluate a candidate's technical skills, problem-solving abilities, and relevant experience to determine if they are a suitable fit for a specific technical role within the company.\nYou should keep the candidate actively engaged and progressing through the given problem. You will provide hints, guidance, and ask probing questions to facilitate the candidate's problem-solving process. \nYou are designed to be supportive, encouraging, and focused on helping the candidate demonstrate their abilities. \n\nYou should ask user to choose question for the technical interview. User can choose specific question or a random one. You can NOT start the interview if you have not received interview question.\nYou should take questions only from question database. You can access question database ONLY by using tools. Do NOT make up questions!\nYou should ask candidate (user) to confirm selected question. For example: \"Are you sure that you want to choose following question: question_name\".\nYou should ask candidate (user) to confirm that he want to end the interview if you think he want. For example: \"Are you sure you want to end the interview?\"\nOnly ask probing questions or give hints if you think the candidate is strugling.\n\n\n**I. Core Principles:**\n\n*   **Facilitating Problem-Solving:** Your focus is on guiding the candidate through the problem, not solving it for them.\n*   **Encouraging Communication:**  Prompt the candidate to explain their thought process and reasoning.\n*   **Providing Strategic Hints:** Offer hints in a graduated manner to help the candidate overcome obstacles.\n*   **Positive and Supportive Tone:**  Create a comfortable environment where the candidate feels encouraged to explore solutions.\n\n**II. Interview Execution Guidance:**\n\n*   **Problem Definition Confirmation:** Ensure the candidate understands the problem statement completely. Ask them to reiterate the problem in their own words. \"So, just to confirm, can you please explain the problem and expected inputs/outputs back to me?\"\n*   **Clarifying Questions:** Encourage the candidate to ask clarifying questions before they start coding. \"What questions do you have about the requirements or constraints before we begin?\"\n*   **Observing and Listening:** Pay close attention to the candidate's thought process.\n*   **Prompting Explanation:**  Actively ask the candidate to \"think out loud.\" Use prompts like:\n    *   \"What are you thinking?\"\n    *   \"What's your next step?\"\n    *   \"Why did you choose that approach?\"\n    *   \"What are the potential trade-offs of this approach?\"\n*   **Code Implementation:** Ask the candidate to code his solution when he described it in words. If candidate first decided to try bruteforce approach, you should still ask him to code it.\n*   **Strategic Hint Provision:** Offer hints in a graduated manner:\n    *   **Level 1 (General):** \"Have you considered [data structure/algorithm]?\", \"What's the time complexity of your current approach?\", \"Are there any edge cases you haven't accounted for?\"\n    *   **Level 2 (Specific):** \"You could use a hash map to [solve specific sub-problem].\", \"Try using a two-pointer approach when [condition exists].\", \"What should happen if the input is null or empty?\"\n    *   **Level 3 (Code Snippet/Conceptual Reminder):** \"In [language], you could use [built-in function] to [achieve specific task].\" \"Remember that Depth First Search involves exploring as far as possible down each branch.\"  Use code snippets SPARINGLY and ONLY after exhausting other options. Focus on conceptual understanding, not direct code completion.\n*   **Redirecting if Necessary:**  If the candidate is completely stuck, gently redirect them to a different approach or a simpler sub-problem.  \"Perhaps we can focus on [simplified task] first, to get a better understanding of the core concept.\"\n\n**Example Interactions:**\n\n*Example 1: Candidate is slightly stuck on approach*\nCandidate Response: \"I'm trying to think how to find the pairs efficiently. Maybe sort the array first?\"\nInterviewer Output/Response: \"Sorting is an interesting idea. What would be the time complexity of sorting, and how would you use the sorted array to find the pair?\" (This is a Level 1 hint, guiding towards complexity and next steps without giving away hash maps yet).\n\n*Example 2: Candidate makes a small logic error in code*\nCandidate Response: \"Here's my code using the hash map.\"\nCandidate Code Snippet: (Python code for Two Sum using hash map, but checks `if target - nums[i] in hm:` *before* putting `nums[i]` in the map)\nInterviewer Output/Response: \"Thanks for sharing the code. It looks like you're correctly using a hash map. Let's trace it with `nums = [3, 2, 4]` and `target = 6`. What happens in the first iteration when `i=0` and `nums[i]=3`? What gets checked in the hash map, and what gets added?\" (This is probing/guiding towards the specific error without explicitly stating it).\n\n*Example 3: Candidate needs a more specific hint*\nCandidate Response: \"I traced it, but I'm still not sure why it wouldn't work for certain cases.\" (After Example 2 trace)\nInterviewer Output/Response: \"Okay, consider the order. Right now, you check if the complement (`target - nums[i]`) exists in the map *before* you add the current number (`nums[i]`) to the map. What if the complement *is* the current number you're processing, like in the `[3, 3]` target 6 example?\" (This is a Level 2 hint pointing more directly at the logic flaw).\n      \n**III. Input Format:**\n\nYou will receive the following inputs:\n\n*   **Candidate Response:** (Transcripts of the candidate's verbal explanations, thinking process, etc.)\n*   **Candidate Code Snippet (optional):** (Optional candidate code snippet. Candidate will provide code only when ready).\n*   **Screanshot of the user whiteboard (optional):** (Optional candidate's whiteboard screenshot. On the whiteboard canidate can explain his thoughts more clearly).\n*   **Instruction:** If a whiteboard image is provided, you MUST analyze its content (diagrams, pseudocode, logic flow, data structures etc.). **Seamlessly integrate your understanding and interpretation of the whiteboard into your main conversational response.** Don't just mention the image exists; refer to *specifics* you observe in it. Use this visual context to:\n*   Confirm your understanding (\"Okay, seeing the diagram confirms you're planning to use a hash map...\").\n        *   Connect it to their words or code (\"That flow you drew on the whiteboard matches your explanation well...\" or \"The way you've depicted the pointers on the whiteboard helps clarify your code's logic...\").\n        *   Ask targeted questions based on the visual (\"Looking at your whiteboard, what happens in the edge case where...?\").\n        *   Point out discrepancies if the visual conflicts with other inputs (\"Your diagram seems to show [X], but in your code, it looks like [Y]. Could you clarify that step?\").\n*   **Crucially, do NOT make up any diagram or explanation, If content of the whiteboard is not related to the currend dialogue or problem you should say \"Content of the whiteboard is not related to the question/conversation\". Do NOT create a separate 'Whiteboard Analysis' section in your output.** Weave these observations naturally into your dialogue with the candidate.\n\n\n**IV. Output Format:**\n\nYour output should contain the following:\n\n**Response:** (Your response to the candidate. You may or may not include probing questions, strategic hints or guiding suggestion to your response.\")\n**Response may include:**\n*   **Next Probing Question:** (Ask a question to encourage the candidate to elaborate on their approach, clarify their reasoning, or identify potential issues. This question might stem from your whiteboard analysis.)\n*   **Strategic Hint (if necessary):** (Provide a hint appropriate for the candidate's current situation, following the Level 1-3 progression. Only provide a hint if the candidate is clearly stuck and not making progress.)\n*   **Guiding Suggestion (if necessary):** (If the candidate needs a nudge, suggest a specific action they could take to move forward. E.g., \"Try writing a simple test case to verify your logic.\")\n\n**Example Input 1 (with Whiteboard):**\n\n```\nCandidate Response: \"Okay, so I'll use a hash map. I'll iterate through the array, and for each element, I'll calculate the complement needed to reach the target. I'll check if the complement is already in the hash map. If it is, I return the indices. If not, I add the current element and its index to the map.\"\nCandidate Code Snippet: (Code implementing the hash map approach, maybe with a small bug)\nScreanshot of the user whiteboard: (Image showing an array [2, 7, 11, 15], target 9, and a box labeled 'HashMap' with arrows indicating lookups and insertions, possibly showing {7: 0} after the first step.)\n```\n\n**Example Output 1 (with Whiteboard Analysis):**\n\n```\n**Response: ** \"That's a good explanation of the hash map approach, and the diagram on your whiteboard clearly illustrates that key idea of storing complements and their indices, like storing {7: 0} after the first step. Your code snippet looks close. Let's trace it with your example [2, 7, 11, 15] and target 9, keeping your diagram in mind. What happens in the code when i is 1 and the element is 7?\"\n**Optional hints:**\n1.  **Next Probing Question:** \"Based on your whiteboard diagram, what value would you expect to find in the hash map when processing the number 7?\"\n2.  **Strategic Hint (if necessary):** Level 2: \"Think about the order of operations. Should you check the hash map before or after adding the current element to it?\"\n3.  **Guiding Suggestion (if necessary):** \"Add a print statement inside your loop to see the state of the hash map at each step.\"\n```\n\n**Example Input 2 (with Whiteboard):**\n\n```\nCandidate Response: \"I think we can iterate over the array to find maximum value, then we should pop this value and do it again. On the third time we will find third biggest number\"\nCandidate Code Snippet: (No code provided))\nScreanshot of the user whiteboard: (Image showing a drawing of a snake that ate an elephant)\n```\n\n**Example Output 2 (with Whiteboard Analysis):**\n\n```\n**Response: ** \"It's great first step to find the right solution but content of the whiteboard is not related to the question. This bruteforce approach could work. Can you elaborate a bit more on this solution.\"\n```\n\n'''\n\nWELCOME_MSG = '''Hello! I'm a technical interviewer for Mock Technologie Inc. I'm here to help you demonstrate your software engineering skills.\n\nTo start, please choose a question for the technical interview. You can either pick a specific question you'd like to work on, or I can select one randomly for you. Let me know what you'd prefer!'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T20:58:21.200656Z","iopub.execute_input":"2025-04-17T20:58:21.200961Z","iopub.status.idle":"2025-04-17T20:58:21.208471Z","shell.execute_reply.started":"2025-04-17T20:58:21.200931Z","shell.execute_reply":"2025-04-17T20:58:21.207696Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"CANDIDATE_EVALUATION_PROMPT = '''\nYour Role: You are an experienced Technical Hiring Manager/Interviewer. Your task is to evaluate a candidate's suitability for a specific technical role based solely on the provided interview transcript.\n\nInputs You Will Be Given:\n\n    [Interview quesiton]: Question that was asked to the candidate.\n    [Interview Transcript]: The full text of the conversation between the interviewer(s) and the candidate.\n    [Candidate code solution]: Final version of the code solution for the problem\n\nYour Task:\n\nAnalyze the provided [Interview Transcript] to assess the candidate's qualifications and fit for the Software Engineer role. Focus on the following key areas, using specific examples and quotes from the transcript as evidence:\n\nEvaluation Criteria:\n    Technical Competence:\n        Problem Understanding: Did they ask clarifying questions? Did they accurately restate the problem and constraints? Did their whiteboard diagram (if provided) accurately reflect the problem?\n        Approach & Algorithm Design: Did they discuss trade-offs (time/space complexity)? Was the chosen approach suitable? Did they break down the problem effectively? Did their whiteboard explanation (if provided) align with their verbal approach and demonstrate logical thinking?\n        Coding & Implementation (as described/discussed): Does their described logic seem sound? Did they discuss data structures/algorithms appropriately? Did they consider edge cases in their plan?\n        Testing & Verification: Did they propose tests or walk through examples (including edge cases)?\n        Debugging & Correction: How did they respond to identified issues or prompts for correction? Could they identify flaws?\n\n    Problem-Solving & Critical Thinking:\n        Systematic Approach: Did they follow a logical process?\n        Adaptability: How did they handle roadblocks or hints? Did they explore alternatives?\n        Optimization: Did they consider performance? Could they articulate why one approach might be better?\n\n    Communication & Collaboration:\n        Clarity of Thought: Could they articulate their thought process clearly while solving problems? Was their explanation easy to follow?\n        Active Listening: Did they seem to understand the interviewer's questions and feedback?\n        Asking Questions: Did they ask relevant clarifying questions about the problem? Did they ask thoughtful questions about the role/team (if applicable in the transcript)?\n        Receiving Feedback: How did they react to constructive criticism or alternative suggestions (receptive, defensive, collaborative)?\n        Professionalism: Was their language professional and respectful?\n\nRequired Output Format:\n\nStructure your evaluation clearly using the following sections:\n    Overall Summary: A brief (2-3 sentence) overview of the candidate's performance and your high-level recommendation (e.g., Strong Hire, Hire, Lean Hire, No Hire, Needs Further Discussion)..\n    Strengths: List 5 key strengths observed during the interview. For each strength, provide specific examples or brief quotes from the transcript as evidence. Evidence should be brief but descriptive. Avoid evidences like \"num[i]\". You can take bigger code snippet with text formating to make accent on the correct/interesting implementation.\n    Areas for Development / Concerns: List atleast 5 key weaknesses or areas where the candidate struggled or raised concerns. For each point, provide specific examples or brief quotes from the transcript as evidence. Avoid evidences like \"num[i]\" if case when it should be nums[j]. You can take bigger code snippet with text formating to make accent on the error.\n    Detailed Analysis: Extensively elaborate on the candidate's performance within each of the main evaluation criteria (Technical, Problem-Solving, Communication), referencing transcript evidence.\n    Final Recommendation & Justification: Clearly state your final hiring recommendation and provide a extensive justification linking back to the key strengths and weaknesses. Give recomendations on what to improve in the context of problem solving and interview communcation.\n    Topics to learn: Provide a list of topics to learn for the user to improve their skills in their areas for development.\n\nImportant Guidelines:\n    Base your entire evaluation strictly on the provided transcript text. Do not infer information not present.\n    Be objective and analytical.\n    Cite specific examples or quote relevant snippets from the transcript to support your points.\n    Maintain a professional tone.\n\n\n# Inputs:\n\n## Interview question\n{question}\n\n## Interview transcript\n{transcript}\n\n## Candidate code solution\n{code}\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T20:58:21.209499Z","iopub.execute_input":"2025-04-17T20:58:21.209753Z","iopub.status.idle":"2025-04-17T20:58:21.232344Z","shell.execute_reply.started":"2025-04-17T20:58:21.209732Z","shell.execute_reply":"2025-04-17T20:58:21.231487Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"RESOURCES_SEARCH_PROMPT = '''\nYou are an expert learning advisor providing recommendations based on a technical interview evaluation.\n\n**Interview Context:**\n*   **Question Asked:** \n\n{question}\n\n*   **Language Used:** {language}\n*   **Expert Evaluation Summary:** \n\n{analytics}\n\n*   **Key Topics Identified for Learning:** \n\n{topics}\n\n**Your Task:**\nGenerate a concise, actionable learning plan or set of recommendations for the candidate based **only** on the provided context and information found via the search tool.\n\n**Instructions:**\n1.  Directly address the \"Topics to Learn\" and relate them to the \"Expert Evaluation Summary\".\n2.  Synthesize information found via search to explain concepts or suggest approaches for improvement.\n3.  Structure your response clearly, perhaps using bullet points for key recommendations.\n4.  **IMPORTANT:** Write a helpful narrative. **Do NOT explicitly list URLs or resource titles in your response text.** The system will automatically add citations based on the search results you utilize.\n5.  Focus on providing clear advice grounded in the search findings.\n6.  Tool Usage: Please use the search tool to find current and relevant resources based on these criteria.\n7.  Skip introduction: Start you answer with concrete plan or recomended actions. Do NOT start you answer with sentences like \"Okay, based on the interview evaluation, here's a...\". Instead start with \"The evaluation indicates weaknesses in coding proficiency, debugging skills...\"\n\n\n\n**Example Output Structure (Conceptual - do not include URLs here):**\n\"Analysis indicates that [Specific Weakness from Analytics], focusing on [Topic 1] is recommended. Understanding the core principles of [Concept Found via Search] can significantly help. For [Topic 2], exploring different approaches like [Approach Found via Search] would be beneficial. Practice problems related to [Relevant Sub-topic] are also advised...\"\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T20:58:21.235114Z","iopub.execute_input":"2025-04-17T20:58:21.235317Z","iopub.status.idle":"2025-04-17T20:58:21.254929Z","shell.execute_reply.started":"2025-04-17T20:58:21.235302Z","shell.execute_reply":"2025-04-17T20:58:21.253644Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"DESCRIBE_IMAGE_PROMPT = '''\nGiven transcript of the technical interview, analyze provided image. \nDescribe its relevancy to the transcript or code (if provided).\nBe concise. Give only required information.\n\n## Transcript:\n{transcript}\n\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T20:58:21.256095Z","iopub.execute_input":"2025-04-17T20:58:21.256463Z","iopub.status.idle":"2025-04-17T20:58:21.278567Z","shell.execute_reply.started":"2025-04-17T20:58:21.256397Z","shell.execute_reply":"2025-04-17T20:58:21.277578Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"### 4.2. Structured Output (Evaluation Report) 📜\n\nTo ensure the evaluation report is consistent and machine-readable, we define Pydantic models that specify the desired JSON schema.\n\n*   **Pydantic Models:** `StrengthItem`, `AreaForDevelopmentItem`, `DetailedAnalysis`, `FinalRecommendation`, `TopicsToLearn`, and the root `EvaluationOutput` model define the exact structure, fields, types, and descriptions for the evaluation data.\n*   **LLM Integration:** When calling Gemini for the evaluation (`create_report_node`), we provide the `EvaluationOutput` model as the `response_schema` and set `response_mime_type` to `application/json`. This instructs Gemini to generate its response adhering strictly to this schema.\n*   **Report Template:** A Jinja2 template (`REPORT_TEMPLATE`) is used to format the structured JSON output into a user-friendly Markdown report.","metadata":{}},{"cell_type":"code","source":"REPORT_TEMPLATE = '''\n# Candidate Interview Evaluation Report\n\n---\n\n## Overall Summary\n\n{{ evaluation.overall_summary }}\n\n---\n\n## Final Recommendation\n\n**Recommendation:** {{ evaluation.final_recommendation.recommendation }}\n\n**Justification:** {{ evaluation.final_recommendation.justification }}\n\n---\n\n## Strengths Observed\n\n{% if evaluation.strengths %}\n  {% for strength in evaluation.strengths %}\n*   **Strength:** {{ strength.point }}\n    *   **Evidence:** {{ strength.evidence }}\n  {% endfor %}\n{% else %}\n*   *No specific strengths noted in the transcript.*\n{% endif %}\n\n---\n\n## Areas for Development / Concerns\n\n{% if evaluation.areas_for_development %}\n  {% for weakness in evaluation.areas_for_development %}\n*   **Area:** {{ weakness.point }}\n    *   **Evidence:** {{ weakness.evidence }}\n  {% endfor %}\n{% else %}\n*   *No specific areas for development or concerns noted in the transcript.*\n{% endif %}\n\n---\n\n## Detailed Analysis\n\n### Technical Competence\n\n{{ evaluation.detailed_analysis.technical_competence }}\n\n### Problem Solving & Critical Thinking\n\n{{ evaluation.detailed_analysis.problem_solving_critical_thinking }}\n\n### Communication & Collaboration\n\n{{ evaluation.detailed_analysis.communication_collaboration }}\n\n---\n\n## Suggested Improvement Resources\n\n*(Note: This section lists suggested resources based on identified development areas. The list might be empty if none were specified.)*\n\n{{ recommendations | default(\"*No specific learning recommendations were generated.*\") }}\n\n---\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T20:58:21.279517Z","iopub.execute_input":"2025-04-17T20:58:21.279732Z","iopub.status.idle":"2025-04-17T20:58:21.300622Z","shell.execute_reply.started":"2025-04-17T20:58:21.279715Z","shell.execute_reply":"2025-04-17T20:58:21.299820Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class StrengthItem(BaseModel):\n    \"\"\"Represents a single observed strength.\"\"\"\n    point: str = Field(\n        ..., # Ellipsis (...) means this field is required\n        description=\"A concise statement describing the specific strength observed (e.g., 'Clear communication of thought process', 'Effective use of data structures', 'Proactively identified edge cases').\"\n    )\n    evidence: str = Field(\n        ...,\n        description=\"Specific examples, observed behaviors, or direct quotes from the transcript that substantiate the identified strength.\"\n    )\n\nclass AreaForDevelopmentItem(BaseModel):\n    \"\"\"Represents a single observed area for development or concern.\"\"\"\n    point: str = Field(\n        ...,\n        description=\"A concise statement describing the specific weakness or area needing improvement (e.g., 'Difficulty explaining time complexity', 'Did not consider null inputs initially', 'Hesitant to ask clarifying questions').\"\n    )\n    evidence: str = Field(\n        ...,\n        description=\"Specific examples, observed behaviors, or direct quotes from the transcript that substantiate the identified weakness or concern.\"\n    )\n\nclass DetailedAnalysis(BaseModel):\n    \"\"\"\n    An object containing extensive, narrative elaboration on the candidate's\n    performance across core evaluation criteria, weaving together observations\n    and transcript evidence.\n    \"\"\"\n    technical_competence: str = Field(\n        ...,\n        alias=\"technicalCompetence\",\n        description=\"Detailed assessment of the candidate's technical skills demonstrated or discussed. Covers aspects like problem understanding, approach formulation, algorithmic thinking, data structure usage, coding logic (as described), testing awareness, and debugging ability, supported by transcript references.\"\n    )\n    problem_solving_critical_thinking: str = Field(\n        ...,\n        alias=\"problemSolvingCriticalThinking\",\n        description=\"Detailed assessment of the candidate's approach to problem-solving. Covers aspects like systematic thinking, logical breakdown of problems, adaptability when facing challenges, response to hints, and consideration of efficiency/optimization, supported by transcript references.\"\n    )\n    communication_collaboration: str = Field(\n        ...,\n        alias=\"communicationCollaboration\",\n        description=\"Detailed assessment of the candidate's communication and interaction style. Covers aspects like clarity of explanation ('thinking out loud'), active listening, ability to articulate ideas, quality of questions asked, receptiveness to feedback, and overall professionalism during the interaction, supported by transcript references.\"\n    )\n\n    class Config:\n        validate_by_name = True\n\n\nclass FinalRecommendation(BaseModel):\n    \"\"\"The concluding hiring decision and the explicit reasoning behind it.\"\"\"\n    recommendation: Literal[\n        \"Strong Hire\", \"Hire\", \"Lean Hire\", \"No Hire\", \"Needs Further Discussion\"\n    ] = Field(\n        ...,\n        description=\"The definitive hiring recommendation category based on the overall evaluation.\"\n    )\n    justification: str = Field(\n        ...,\n        description=\"A concise summary justifying the final recommendation. It should explicitly link the key strengths and areas for development to the requirements of the specific role and level being considered.\"\n    )\n\n\nclass TopicsToLearn(BaseModel):\n    \"\"\"Represents a single suggested learning resource.\"\"\"\n    area: str = Field(\n        ...,\n        description=\"The specific skill or knowledge area the resource is intended to address (e.g., 'Algorithm Complexity Analysis', 'Test-Driven Development Principles', 'Effective Technical Communication'). Should ideally correspond to one or more points in 'areasForDevelopment'.\"\n    )\n    description: str = Field(\n        ...,\n        description=\"A brief explanation of the resource content or why it is being recommended in relation to the candidate's development needs.\"\n    )\n\n# Define the main evaluation output model\n\nclass EvaluationOutput(BaseModel):\n    \"\"\"\n    Root object containing the complete candidate evaluation derived\n    from the interview transcript.\n    \"\"\"\n    overall_summary: str = Field(\n        ...,\n        alias=\"overallSummary\",\n        description=\"A brief (2-3 sentence) overview of the candidate's performance and a high-level hiring recommendation (e.g., Strong Hire, Hire, Lean Hire, No Hire, Needs Further Discussion). This provides a quick snapshot of the evaluation outcome.\"\n    )\n    strengths: List[StrengthItem] = Field(\n        ...,\n        description=\"An array listing the key positive attributes, skills, and behaviors demonstrated by the candidate during the interview. The number of items in the array will vary based on the interview content.\"\n    )\n    areas_for_development: List[AreaForDevelopmentItem] = Field(\n        ...,\n        alias=\"areasForDevelopment\", \n        description=\"An array listing the key weaknesses, areas where the candidate struggled, skills gaps, or concerns identified during the interview. The number of items in the array will vary.\"\n    )\n    detailed_analysis: DetailedAnalysis = Field(\n        ...,\n        alias=\"detailedAnalysis\",\n        description=\"Extensive analysis of the candidate's performance within each of the main evaluation criteria (Technical, Problem-Solving, Communication), referencing transcript evidence. Each criterial should be atleast 5 sentences.\"\n    )\n    final_recommendation: FinalRecommendation = Field(\n        ...,\n        alias=\"finalRecommendation\"\n    )\n    topics_to_learn: List[TopicsToLearn] = Field(\n        ..., # Ellipsis marks it as required\n        alias=\"topicsToLearn\",\n        description=\"A required array of suggested learning topics targeted at the candidate's identified 'Areas for Development'. The list can be empty if no specific topics are suggested, but the field must be present.\"\n    )\n\n    class Config:\n        validate_by_name = True # Allows using either snake_case or alias for input\n        json_schema_extra = {\n            \"title\": \"Candidate Interview Evaluation\",\n            \"description\": \"A structured evaluation of a candidate based on an interview transcript.\"\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T20:58:21.301382Z","iopub.execute_input":"2025-04-17T20:58:21.301660Z","iopub.status.idle":"2025-04-17T20:58:21.328188Z","shell.execute_reply.started":"2025-04-17T20:58:21.301640Z","shell.execute_reply":"2025-04-17T20:58:21.327571Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"### 4.3. Utility Functions 🛠️\n\nSeveral helper functions support the agent's operation:\n\n*   `encode64_pil_image`: Converts PIL Images (from Gradio Sketchpad) to base64 strings suitable for inclusion in Gemini API calls.\n*   `get_data_for_search`: Extracts structured analysis and learning topics from the evaluation output to feed into the grounding prompt.\n*   `get_interview_transcript`: Processes the message history (`InterviewState['messages']`), extracting text and code. If an image is present, it calls Gemini using `DESCRIBE_IMAGE_PROMPT` to get a text description and incorporates it, creating a comprehensive transcript for the final evaluation. This handles the **long context** aspect.\n*   `get_learning_resources`: Implements the **Grounding** feature. It calls Gemini with the `RESOURCES_SEARCH_PROMPT`, enables the Google Search tool (`types.Tool(google_search=types.GoogleSearch())`), processes the response to extract the generated text and citation metadata, and formats it into a Markdown string with superscript citations and a reference list. Includes retry logic for robustness.","metadata":{}},{"cell_type":"code","source":"client = genai.Client(api_key=UserSecretsClient().get_secret(\"GOOGLE_API_KEY\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T20:58:21.328820Z","iopub.execute_input":"2025-04-17T20:58:21.328985Z","iopub.status.idle":"2025-04-17T20:58:21.675396Z","shell.execute_reply.started":"2025-04-17T20:58:21.328972Z","shell.execute_reply":"2025-04-17T20:58:21.674785Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"Although Google API [supports using requests with PIL images](https://ai.google.dev/gemini-api/docs/image-understanding#upload-image) as it is, when using a langchain wrapper we can not add PIL images to the message. Therefore I use this function to convert PIL image to base64 format.","metadata":{}},{"cell_type":"code","source":"def encode64_pil_image(img: PIL.Image.Image) -> str:\n    \"\"\"Converts PIL Image to base64 format\"\"\"\n    buffer = BytesIO()\n    img.save(buffer, format=\"PNG\")\n    image_bytes = buffer.getvalue()\n    base64_image = base64.b64encode(image_bytes).decode('utf-8')\n    return base64_image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T20:58:21.676072Z","iopub.execute_input":"2025-04-17T20:58:21.676252Z","iopub.status.idle":"2025-04-17T20:58:21.680239Z","shell.execute_reply.started":"2025-04-17T20:58:21.676236Z","shell.execute_reply":"2025-04-17T20:58:21.679752Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def get_data_for_search(response: EvaluationOutput) -> Tuple[str, str]:\n    \"\"\"Gets analysis and topics from response and returns them as separate strings\"\"\"\n    analytics = \"\"\n    for item in response.parsed.detailed_analysis:\n        theme, desc = item\n        analytics += f\"{theme}: {desc}\\n\\n\"\n    \n    topics = \"\"\n    for item in response.parsed.topics_to_learn:\n        topics += f\"{item.area}: {item.description}\\n\\n\"\n\n    return analytics, topics\n\n\ndef get_interview_transcript(messages: List[BaseMessage]) -> str:\n    \"\"\"\n    Read messages from history, extracts message text, code and transcribing images.\n    Returns dialogue transcript\n    \"\"\"\n    transcript: str = \"\"\n    image_url: Optional[Dict[str, str]] = None\n\n    for message in messages:\n        if isinstance(message, AIMessage):\n            if message.content:\n                transcript += f\"Interviewer: {message.content}\\n\\n\"\n                \n        if isinstance(message, HumanMessage):\n            text: str = \"\"\n            for content in message.content:\n                content: Dict[str, Any]\n                text += content.get(\"text\", \"\") + \"\\n\"\n                if image_url_data := content.get(\"image_url\", None):\n                    image_url: Dict[str, str] = image_url_data\n                    response = client.models.generate_content(\n                        model='gemini-2.0-flash',\n                        contents=[DESCRIBE_IMAGE_PROMPT.format(transcript=transcript), image_url.get(\"url\")],\n                    )\n                    text += f\"Image description:\\n{response.text}\\n\"\n                    \n            transcript += f\"Candidate: {text}\\n\\n\"\n    return transcript","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T20:58:21.681161Z","iopub.execute_input":"2025-04-17T20:58:21.681397Z","iopub.status.idle":"2025-04-17T20:58:21.699616Z","shell.execute_reply.started":"2025-04-17T20:58:21.681371Z","shell.execute_reply":"2025-04-17T20:58:21.698727Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"**Here is the logic of this function:**\n\n1. Get generation using client and `RESOURCES_SEARCH_PROMPT` prompt.\n2. If the response contains grounding metadata we can continue execution.\n> When search grounding is used, the model returns extra metadata that includes links to search suggestions, supporting documents and information on how the supporting documents were used. [Get more info here](https://www.kaggle.com/code/markishere/day-4-google-search-grounding#Use-search-grounding).\n\n3. If the response does not contain grounding metadata -> retry. The number of retries is limited by the num_retries variable.\n4. After successful generation with grounding metadata we take each supported generated chunk and cite it.\n5. Do this for every chunk and combine it for a full cited response.","metadata":{}},{"cell_type":"code","source":"def get_learning_resources(question: str, analytics: str, topics: str, language: str) -> str:\n    \"\"\"Using Gemini model generates learning plan with grounding\"\"\"\n    config_with_search = types.GenerateContentConfig(\n        tools=[types.Tool(google_search=types.GoogleSearch())]\n    )\n    \n    markdown_parts: List[str] = []\n    num_retries: int = 3\n    rc: Optional[types.Candidate] = None \n\n    # Run for only num_retries times so it is not infinite\n    while num_retries > 0:\n        try:\n            response = client.models.generate_content(\n                model='gemini-2.0-flash',\n                contents=RESOURCES_SEARCH_PROMPT.format(question=question, analytics=analytics, topics=topics, language=language),\n                config=config_with_search\n            )\n            rc = response.candidates[0]\n\n            # Check for grounding metadata existence\n            if (rc.grounding_metadata\n                and rc.grounding_metadata.grounding_supports\n                and rc.grounding_metadata.grounding_chunks\n                and rc.content.parts # Ensure there's content generated\n                and rc.content.parts[0].text):\n                 break # Success!\n\n            print(f\"Retrying grounding query... ({3 - num_retries + 1})\")\n            num_retries -= 1\n            \n        except Exception as e:\n             print(f\"An error occurred during grounding query: {e}\")\n             num_retries -= 1 \n\n        if num_retries <= 0:\n            print(\"Failed to get grounded recommendations after multiple retries.\")\n            text_without_citations: str = \"\\n\".join([part.text for part in rc.content.parts])\n            return f\"*Could not retrieve grounded learning recommendations at this time.*\\n\\n{text_without_citations}\" # Fallback message\n    \n    if rc and rc.grounding_metadata:\n        supports: List[types.GroundingSupport] = rc.grounding_metadata.grounding_supports\n        chunks: List[types.GroundingChunk] = rc.grounding_metadata.grounding_chunks\n        # Combine all parts of the text generation to properly citate\n        generated_text: str = \"\\n\".join([part.text for part in rc.content.parts])\n        \n        last_index: int = 0 # Initialize last_index to 0 to add first part of the generated text\n        for support in sorted(supports, key=lambda s: s.segment.start_index):\n            # Write text before the current support segment\n            markdown_parts.append(generated_text[last_index : support.segment.start_index])\n\n            # Write the supported text segment\n            supported_segment: str = generated_text[support.segment.start_index : support.segment.end_index]\n            markdown_parts.append(supported_segment)\n\n            # Add citation markers for this segment\n            citation_indices: List[int] = sorted(list(set(support.grounding_chunk_indices))) # Unique, sorted indices\n            for i in citation_indices:\n                  markdown_parts.append(f\"<sup>[{i+1}]</sup>\")\n\n            last_index = support.segment.end_index \n\n        # Write any remaining text after the last support segment\n        markdown_parts.append(generated_text[last_index:])\n        markdown_parts.append(\"\\n\\n\")\n\n        # Print the citations\n        if chunks:\n            markdown_parts.append(\"### Citations:\\n\\n\")\n            for i, chunk in enumerate(chunks, start=1):\n                title: str = chunk.web.title or \"Untitled\"\n                uri: str = chunk.web.uri or \"#\"\n                markdown_parts.append(f\"{i}. [{title}]({uri})\\n\")\n        else:\n             markdown_parts.append(\"*No specific web sources were cited for this recommendation.*\\n\")\n\n        return \"\".join(markdown_parts)\n    else:\n         # Fallback if somehow rc is None or grounding metadata is missing after loop\n         return \"*Could not retrieve grounded learning recommendations.*\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T20:58:21.700383Z","iopub.execute_input":"2025-04-17T20:58:21.700644Z","iopub.status.idle":"2025-04-17T20:58:21.729789Z","shell.execute_reply.started":"2025-04-17T20:58:21.700625Z","shell.execute_reply":"2025-04-17T20:58:21.728817Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"### 4.4. LangGraph State & LLM 🧠\n\n*   **`InterviewState` (TypedDict):** Defines the information that persists and flows through the graph. It includes `messages` (the conversation history, managed by `add_messages`), the current `question`, the candidate's latest `code`, and a `finished` flag.\n*   **LLM Initialization:** We initialize the `ChatGoogleGenerativeAI` model (`gemini-2.0-flash`) which will be used for the main conversational turns.","metadata":{}},{"cell_type":"code","source":"class InterviewState(TypedDict):\n    \"\"\"State representing the customer's order conversation.\"\"\"\n    messages: Annotated[list, add_messages]\n    question: str\n    code: str\n    finished: bool","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T20:58:21.730700Z","iopub.execute_input":"2025-04-17T20:58:21.730974Z","iopub.status.idle":"2025-04-17T20:58:21.754126Z","shell.execute_reply.started":"2025-04-17T20:58:21.730951Z","shell.execute_reply":"2025-04-17T20:58:21.753247Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T20:58:21.754850Z","iopub.execute_input":"2025-04-17T20:58:21.755016Z","iopub.status.idle":"2025-04-17T20:58:21.863486Z","shell.execute_reply.started":"2025-04-17T20:58:21.755003Z","shell.execute_reply":"2025-04-17T20:58:21.862615Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"### 4.5. Tools & Function Calling 🧰\n\nTools allow the agent to interact with external resources (like the question database).\n\n*   **Tool Definitions:** Functions like `select_question`, `list_questions`, `get_random_problem`, `get_difficulty_levels`, `get_topic_categories`, and `end_interview` are decorated with `@tool`. Pydantic models (`ListQuestionArgs`, `SelectQuestionArgs`) define the expected arguments for some tools. These tools interact with the Pandas DataFrame (`df`) loaded earlier or manipulate the interview state.\n*   **Binding Tools:** The tools are bound to the LLM (`llm_with_tools = llm.bind_tools(...)`). This allows the LLM to decide to call one or more of these functions and generate the necessary arguments in its response (`tool_calls`).\n*   **`ToolNode`:** A LangGraph node specifically designed to execute the function calls requested by the LLM.\n*   **Custom Tool Handling:** Specific tools (`select_question`, `end_interview`) require custom nodes (`question_selection_node`, `finish_interview_node`) to manipulate the `InterviewState` (e.g., update the current question, set the `finished` flag) based on the tool's execution, rather than just returning a simple text result.","metadata":{}},{"cell_type":"code","source":"DIFFICULTY = tuple(df.difficulty.unique().tolist())\nTOPICS = tuple(df.topic.unique().tolist())\nIDS = df.id.apply(str).tolist()\n\n\nclass ListQuestionArgs(BaseModel):\n    \"\"\"Input schema for the list_questions tool.\"\"\"\n    category: Literal[TOPICS] = Field(\n        description=\"The topic category to filter by\"\n    )\n    difficulty: Literal[DIFFICULTY] = Field(\n        description=\"The difficulty level to filter by (e.g., 'Easy', 'Medium', 'Hard').\"\n    )\n\n\nclass SelectQuestionArgs(BaseModel):\n    \"\"\"Input schema for the select_question tool.\"\"\"\n    ID: Literal[tuple(IDS)] = Field(description=\"ID of the question\")\n\n\n\n@tool(args_schema=SelectQuestionArgs)\ndef select_question(ID: str) -> str:\n    \"\"\"Shows user question with provided ID.\n    ALWAYS use this tool when the candidate confirms selected question.\n    You can start interview process ONLY after using this tool.\n    \"\"\"\n\n\n@tool(args_schema=ListQuestionArgs)\ndef list_questions(category: str, difficulty: str) -> Union[str, List[str]]:\n    \"\"\"\n    Lists and returns question names and IDs, filtering by category and difficulty.\n    \"\"\"\n    filtered_df = df[(df['topic'].str.lower() == category.lower()) & (df['difficulty'].str.lower() == difficulty.lower())]\n    if filtered_df.empty:\n        return f\"There is no questions with the topic '{category}' and difficulty '{difficulty}'\"\n\n    questions_to_sample = min(len(filtered_df), 5)\n    filtered_sampled_df = filtered_df.sample(n=questions_to_sample)\n    \n    questions = []\n    for index, row in filtered_sampled_df[[\"id\", \"problem_name\"]].iterrows():\n        questions.append(f\"ID: {row.id}; Problem name: {row.problem_name}\")\n        \n    return questions\n\n\n@tool\ndef get_random_problem() -> str:\n    \"\"\"Selects and returns a random question description.\"\"\"\n    try:\n        random_problem = df.sample(n=1).iloc[0]\n    except ValueError as e:\n         return f\"Error selecting problem after filtering: {e}\"\n\n    question = f\"ID: {random_problem.id} Problem name: {random_problem.problem_name} \"\n    return question\n\n\n@tool\ndef get_difficulty_levels() -> List[str]:\n    \"\"\"Provides information about difficulty levels in question database.\n\n    Returns:\n      List of available difficulty levels in question database\n    \"\"\"\n    return df.difficulty.unique().tolist()\n\n\n@tool\ndef get_topic_categories() -> List[str]:\n    \"\"\"Provides list of topics in question database.\n\n    Returns:\n      List of available topics in question database\n    \"\"\"\n    return df.topic.unique().tolist()\n\n\n@tool\ndef end_interview() -> bool:\n    \"\"\"Ends interview, starts evaluation process of the candidate. Use this when the user said goodbye.\n    \"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T20:58:21.864236Z","iopub.execute_input":"2025-04-17T20:58:21.864504Z","iopub.status.idle":"2025-04-17T20:58:21.885085Z","shell.execute_reply.started":"2025-04-17T20:58:21.864484Z","shell.execute_reply":"2025-04-17T20:58:21.884381Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"Here we are are using two types of tools: auto_tools and interview_tools. This is because langraph doesn't allow tools to change state. Therefore we need to create special nodes to execute such functions. ","metadata":{}},{"cell_type":"code","source":"auto_tools: List[BaseTool] = [get_difficulty_levels, get_topic_categories, get_random_problem, list_questions]\ntool_node = ToolNode(auto_tools)\n\ninterview_tools: List[BaseTool] = [select_question, end_interview]\nllm_with_tools: Runnable = llm.bind_tools(auto_tools + interview_tools)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T20:58:21.885740Z","iopub.execute_input":"2025-04-17T20:58:21.885919Z","iopub.status.idle":"2025-04-17T20:58:21.920346Z","shell.execute_reply.started":"2025-04-17T20:58:21.885905Z","shell.execute_reply":"2025-04-17T20:58:21.919364Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"### 4.6. Graph Nodes & Edges ⚫️🔗🔘\n\nThe interview flow is defined by the nodes and edges in the LangGraph:\n\n*   **Nodes:**\n    *   `chatbot`: The main node where the LLM is called with the current state (messages, system prompt) to generate the next response or request a tool call.\n    *   `tools`: Executes standard tool calls identified by the chatbot node (e.g., `list_questions`).\n    *   `question selection`: Custom node to handle the `select_question` tool call and update the state.\n    *   `end interview`: Custom node to handle the `end_interview` tool call, set the `finished` flag, and trigger the report generation.\n    *   `create report`: Node called after `end interview`. It generates the evaluation using structured output and the learning recommendations using grounding, then formats the final report.\n*   **Edges:**\n    *   `START`: The graph begins at the `chatbot` node.\n    *   Conditional Edges (`maybe_route_to_tools`): After the `chatbot` node runs, this function checks the LLM's output. If `tool_calls` are present, it routes the flow to the appropriate tool node (`tools`, `question selection`, or `end interview`). If no tool calls are present, or after a tool has run, the graph might end (`__end__`), waiting for the next user input.\n    *   The flow ensures that tool results are processed, state is updated, and the final report is generated upon completion.","metadata":{}},{"cell_type":"code","source":"def chatbot_with_tools(state: InterviewState) -> InterviewState:\n    \"\"\"The chatbot node. Invokes the LLM with tools.\"\"\"\n    print(\"\\nEntering chatbot_with_tools Node\")\n    messages: List[BaseMessage] = state['messages']\n\n    system_and_messages: List[BaseMessage] = [SystemMessage(content=INTERVIEWER_SYSTEM_PROMPT)] + messages\n\n    if not messages:\n        ai_message = AIMessage(content=WELCOME_MSG)\n    else:\n        ai_message = llm_with_tools.invoke(system_and_messages)\n\n    return state | {\"messages\": [ai_message]}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T20:58:21.921112Z","iopub.execute_input":"2025-04-17T20:58:21.921329Z","iopub.status.idle":"2025-04-17T20:58:21.937763Z","shell.execute_reply.started":"2025-04-17T20:58:21.921310Z","shell.execute_reply":"2025-04-17T20:58:21.936787Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"The `question_selection_node` uses ID to select a question. ID is provided by the modes in the tool arguments. The model knows the ID of a specific question from the tool that it called previously. Like `list_questions` and `get_random_question`. They return the question name and question ID.","metadata":{}},{"cell_type":"code","source":"def question_selection_node(state: InterviewState) -> InterviewState:\n    \"\"\"The question selection node. This is where the interview state is manipulated.\"\"\"\n    \n    tool_msg: AIMessage = state.get(\"messages\", [])[-1]\n    question: str = state.get(\"question\", \"\")\n    outbound_msgs: List[ToolMessage] = []\n    tool_call: Dict[str, str]\n\n    for tool_call in tool_msg.tool_calls:\n\n        if tool_call[\"name\"] == \"select_question\":\n\n            ID: int = int(tool_call[\"args\"][\"ID\"])\n            \n            selected_question: pd.Series = df[df.id==ID].iloc[0]\n\n            question = selected_question.content\n            question_code: str = selected_question.code\n            instruction: str = \"Following question description is used to show you what this question is about. You should not repeat it to the user!\"\n            response: str = \"\\n\".join([instruction] + [question] + [f\"Initial code: {question_code}\"])\n\n        else:\n            raise NotImplementedError(f'Unknown tool call: {tool_call[\"name\"]}')\n\n        outbound_msgs.append(\n            ToolMessage(\n                content=response,\n                name=tool_call[\"name\"],\n                tool_call_id=tool_call[\"id\"],\n            )\n        )\n\n    return state | {\"messages\": outbound_msgs, \"question\": question, \"code\": question_code}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T20:58:21.938841Z","iopub.execute_input":"2025-04-17T20:58:21.939525Z","iopub.status.idle":"2025-04-17T20:58:21.957337Z","shell.execute_reply.started":"2025-04-17T20:58:21.939501Z","shell.execute_reply":"2025-04-17T20:58:21.956280Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"def finish_interview_node(state: InterviewState) -> InterviewState:\n    \"\"\"The question selection node. This is where the interview state is manipulated.\"\"\"\n    \n    tool_msg: AIMessage = state.get(\"messages\", [])[-1]\n    finished: bool = state.get(\"finished\", False)\n    outbound_msgs: List[ToolMessage] = []\n    tool_call: Dict[str, str]\n\n    for tool_call in tool_msg.tool_calls:\n\n        if tool_call[\"name\"] == \"end_interview\":\n            print(\"FINISH TOOL\")\n            finished = True\n            response: str = \"Say goodbye to the user\"\n\n        else:\n            raise NotImplementedError(f'Unknown tool call: {tool_call[\"name\"]}')\n\n        outbound_msgs.append(\n            ToolMessage(\n                content=response,\n                name=tool_call[\"name\"],\n                tool_call_id=tool_call[\"id\"],\n            )\n        )\n\n    return state | {\"messages\": outbound_msgs, \"finished\": finished}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T20:58:21.958178Z","iopub.execute_input":"2025-04-17T20:58:21.958454Z","iopub.status.idle":"2025-04-17T20:58:21.980294Z","shell.execute_reply.started":"2025-04-17T20:58:21.958412Z","shell.execute_reply":"2025-04-17T20:58:21.979476Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"The `create_report_node` first calls Google API to generate a structured report using the `EvaluationOutput` schema. Then it calls the `get_learning_resources` function to generate a grounded learning plan to insert at the end of the report. After that, it renders this data with the jinja2 template, saves the file, and updates the state.","metadata":{}},{"cell_type":"code","source":"def create_report_node(state: InterviewState) -> InterviewState:\n    question: str = state.get(\"question\", \"\")\n    if not question or question == \"Problem has not been selected yet\": \n        print(\"No valid question selected. Skipping report generation.\")\n        return state | {\"report\": \"Report cannot be generated as no question was selected or the interview did not complete.\"}\n        \n    messages: List[BaseMessage] = state.get(\"messages\", [])\n    transcript: str = get_interview_transcript(messages)\n    code: str = state.get(\"code\", \"\")\n    \n    try:\n        evaluation_response = client.models.generate_content(\n            model='gemini-2.0-flash',\n            contents=CANDIDATE_EVALUATION_PROMPT.format(question=question, transcript=transcript, code=code),\n            config={\n                'response_mime_type': 'application/json',\n                'response_schema': EvaluationOutput,\n            },\n        )\n        evaluation_data: EvaluationOutput = evaluation_response.parsed\n    except Exception as e:\n        print(f\"Error generating candidate evaluation: {e}\")\n        return state | {\"report\": f\"Error generating evaluation report: {e}\"}\n\n    analytics: str\n    topics: str\n    analytics, topics = get_data_for_search(evaluation_response)\n    grounded_recommendations_md: str = get_learning_resources(question, analytics, topics, \"Python\")\n    \n    template: Template = Template(REPORT_TEMPLATE)\n    rendered_output: str = template.render({\"evaluation\": evaluation_data, \"recommendations\": grounded_recommendations_md})\n\n    try:\n        with open(\"candidate_report.md\", \"w\", encoding=\"utf-8\") as f:\n            f.write(rendered_output)\n        print(\"Candidate report generated successfully.\")\n    except IOError as e:\n        print(f\"Error writing report file: {e}\")\n        \n    return state | {\"report\": rendered_output}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T20:58:21.981286Z","iopub.execute_input":"2025-04-17T20:58:21.981609Z","iopub.status.idle":"2025-04-17T20:58:22.003597Z","shell.execute_reply.started":"2025-04-17T20:58:21.981581Z","shell.execute_reply":"2025-04-17T20:58:22.002713Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"def maybe_route_to_tools(state: InterviewState) -> Literal[\"tools\", \"question selection\", \"end interview\", \"__end__\"]:\n    \"\"\"Routes to tools if the LLM's last message contained tool calls.\"\"\"\n    msgs: List[BaseMessage]\n    tool: Dict\n    \n    if not (msgs := state.get(\"messages\", [])):\n        raise ValueError(f\"No messages found when parsing state: {state}\")\n        \n    last_message: BaseMessage = state['messages'][-1]\n    if hasattr(last_message, \"tool_calls\") and len(last_message.tool_calls) > 0:\n        print(f\"Routing to tools based on message: {last_message.tool_calls}\")\n        # Route to `tools` node for any automated tool calls first.\n        if any(\n            tool[\"name\"] in tool_node.tools_by_name.keys() for tool in last_message.tool_calls\n        ):\n            return \"tools\"\n        elif any(\n            tool[\"name\"] == \"select_question\" for tool in last_message.tool_calls\n        ):\n            return \"question selection\"\n        elif any(\n            tool[\"name\"] == \"end_interview\" for tool in last_message.tool_calls\n        ):\n            return \"end interview\"\n        else:\n            return \"__end__\"\n    else:\n        print(\"No tool calls found. Ending graph run (will wait for user input).\")\n        return \"__end__\" # End the current graph invocation","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T20:58:22.004520Z","iopub.execute_input":"2025-04-17T20:58:22.004733Z","iopub.status.idle":"2025-04-17T20:58:22.029114Z","shell.execute_reply.started":"2025-04-17T20:58:22.004717Z","shell.execute_reply":"2025-04-17T20:58:22.028396Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"### 4.7. Graph Compilation & Visualization 🖼️\n\nThe defined nodes and edges are compiled into an executable LangGraph object (`interviewer_graph`).","metadata":{}},{"cell_type":"code","source":"graph_builder = StateGraph(InterviewState)\n\ngraph_builder.add_node(\"chatbot\", chatbot_with_tools)\ngraph_builder.add_node(\"tools\", tool_node)\ngraph_builder.add_node(\"question selection\", question_selection_node)\ngraph_builder.add_node(\"end interview\", finish_interview_node)\ngraph_builder.add_node(\"create report\", create_report_node)\n\n\ngraph_builder.add_edge(START, \"chatbot\")\ngraph_builder.add_conditional_edges(\n    \"chatbot\",\n    maybe_route_to_tools,\n)\n# After tools are executed, always return to the chatbot to process the tool results\ngraph_builder.add_edge(\"tools\", \"chatbot\")\ngraph_builder.add_edge(\"question selection\", \"chatbot\")\ngraph_builder.add_edge(\"end interview\", \"create report\")\ngraph_builder.add_edge(\"create report\", \"__end__\")\ninterviewer_graph = graph_builder.compile()\ntry:\n    Image(interviewer_graph.get_graph().draw_mermaid_png())\nexcept requests.exceptions.ReadTimeout as e:\n    print(\"Could not draw graph...\", e)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T20:58:22.029870Z","iopub.execute_input":"2025-04-17T20:58:22.030089Z","iopub.status.idle":"2025-04-17T20:58:23.344523Z","shell.execute_reply.started":"2025-04-17T20:58:22.030070Z","shell.execute_reply":"2025-04-17T20:58:23.343733Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"## ✨ 5. Showcasing GenAI Capabilities\n\nThis project explicitly demonstrates several powerful Generative AI features available through the Gemini API and LangChain/LangGraph:\n\n1.  **Function Calling:** The interviewer agent uses tools (`list_questions`, `select_question`, `get_random_problem`, `end_interview`, etc.) defined with the `@tool` decorator. The LLM (`llm_with_tools`) decides when to call these functions based on the conversation and system prompt, generating the required arguments. This allows the agent to interact with the external question database and control the interview flow (e.g., ending the interview). The `ToolNode` and custom tool nodes handle the execution.\n\n2.  **Structured Output:** The `create_report_node` leverages Gemini's ability to generate JSON output conforming to a predefined schema. By defining Pydantic models (`EvaluationOutput`, etc.) and specifying them in the `generate_content` call (`response_schema`), we ensure the evaluation report is consistently structured, making it easy to parse and format using the Jinja2 template (`REPORT_TEMPLATE`).\n\n3.  **Few-Shot Learning:** The `INTERVIEWER_SYSTEM_PROMPT` includes explicit examples of interactions (e.g., how to provide hints, how to respond to whiteboard input). This provides the LLM with concrete examples of the desired behavior, improving its ability to follow instructions and adopt the correct persona and interaction style.\n\n4.  **Image Understanding (Multimodality):** The application accepts whiteboard sketches via the Gradio `Sketchpad`. The `chat_interface_update` function encodes the PIL image to base64. The `get_interview_transcript` function then incorporates this image data when creating the full transcript passed to the evaluation model. It uses a Gemini Vision model via `DESCRIBE_IMAGE_PROMPT` to generate a textual description of the image, integrating visual information into the context.\n\n5.  **Agents (LangGraph):** The entire interviewer is built as a stateful agent using LangGraph. It maintains the conversation state (`InterviewState`), transitions between different processing steps (nodes like `chatbot`, `tools`, `create_report`), and makes decisions based on LLM outputs and tool results (conditional edges via `maybe_route_to_tools`). This enables the complex, multi-turn, and tool-using behavior required for the simulation.\n\n6.  **Long Context Window:** The agent handles potentially long interview conversations. The `InterviewState` accumulates all messages. The `get_interview_transcript` function processes this entire history, and the `create_report_node` passes the full transcript to the evaluation model, allowing it to consider the entire interaction when generating the assessment. Gemini's long context capabilities are essential here.\n\n7.  **Grounding (Gemini with Google Search):** The `get_learning_resources` function implements grounding. It calls Gemini with a specific prompt (`RESOURCES_SEARCH_PROMPT`) and enables the `GoogleSearch` tool. Gemini uses search results to generate a *grounded* learning plan based on the evaluation feedback and identified topics, ensuring the recommendations are relevant and based on real-world information. The system automatically handles citations for the sources used.","metadata":{}},{"cell_type":"markdown","source":"## 🕹️ 6. Gradio User Interface\n\nTo make the Mock Interviewer interactive, I build a web interface using Gradio.\n\n### 6.1. Interaction Logic ↔️\n*   **State Management (`gr.State`):** A Gradio `State` object (`chat_state`) holds the current `InterviewState` (messages, question, code, finished flag) and UI-related flags (`code_changed`, `image_changed`) between interactions.\n*   **Initialization (`init_chat`):** When the Gradio app loads, this function runs the LangGraph with an empty initial state to get the welcome message from the `chatbot` node and sets up the initial UI components.\n*   **Update Loop (`chat_interface_update`):** This is the core function triggered by the \"Send Message\" button.\n    1.  It retrieves the current state and user inputs (text, code, image).\n    2.  It checks if the interview is already marked as finished.\n    3.  It packages the user's input (text, code, image description) into a `HumanMessage`. Special handling ensures code and images are included only if they've changed since the last turn. Images are base64 encoded.\n    4.  It appends the new `HumanMessage` to the state's message list.\n    5.  It invokes the `interviewer_graph` with the updated state.\n    6.  It processes the `new_state` returned by the graph:\n        *   Extracts the latest AI message to display.\n        *   Updates the problem description and code editor content if they changed.\n        *   Checks the `finished` flag in the new state.\n        *   Updates the `chat_state` for the next turn.\n        *   Updates the visibility of the Download button based on the `finished` flag.\n        *   Returns updates for all relevant Gradio components.\n*   **Input Change Handlers:** `mark_code_as_changed` and `mark_image_as_changed` are triggered by changes in the `gr.Code` and `gr.Sketchpad` inputs, respectively. They update the corresponding data and flags in the `chat_state` without triggering a full graph invocation, ensuring that code/image changes are only sent when the user explicitly clicks \"Send Message\".","metadata":{}},{"cell_type":"code","source":"def chat_interface_update(user_input: str, chat_state: dict) -> Tuple:\n    \"\"\"\n    Handles user input, runs the LangGraph, and returns the AI response and updated state.\n    \"\"\"\n    print(f\"\\n--- Gradio Interaction Start ---\")\n    REPORT_FILENAME = \"candidate_report.md\"\n\n    # Check if state exists and initialize it if not\n    if not chat_state or 'messages' not in chat_state:\n        chat_state = {\n            \"messages\": [], \n            \"question\": \"Problem has not been selected yet\", \n            \"code\": \"# Your code\", \n            \"code_changed\": False, \n            \"image_changed\": False,\n            \"finished\": False\n        }\n    \n    current_messages: str = chat_state.get('messages', [])\n    current_problem: str = chat_state.get('question', \"Problem has not been selected yet\")\n    current_code: str = chat_state.get('code', \"# Your code\")\n    current_image: Optional[PIL.Image] = chat_state.get('image', None)\n    code_changed: bool = chat_state.get('code_changed', False)\n    image_changed: bool = chat_state.get('image_changed', False)\n    finished: bool = chat_state.get('finished', False)\n\n    # Check if finished to display predifined message and show Download report button\n    if finished:\n        print(\"Interview already marked as finished.\")\n        final_message = \"Interview is finished. Your report is ready for download below.\"\n        return (\n            gr.update(value=final_message),    # output_text\n            gr.update(),                       \n            gr.update(),                       \n            chat_state,                        \n            gr.update(visible=True),\n            gr.update(visible=False),\n        )\n\n    # Initializing content to pass text, code and image (if included)\n    content: List[Dict[str, Any]] = []\n    \n    if user_input:\n        content.append({\"type\": \"text\", \"text\": user_input})\n    \n    if code_changed:\n        content.append({\"type\": \"text\", \"text\": f\"\\nCode:```python\\n{current_code}\\n```\"})\n\n    if image_changed:\n        # Get converted to base64 image\n        base64_image: str = encode64_pil_image(current_image)\n        content.append({\"type\": \"text\", \"text\": \"Here is a screenshot of my whiteboard\"})\n        content.append({\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}})\n\n    # Check for input\n    if content:\n        current_messages.append(HumanMessage(content=content))\n        chat_state['code_changed'] = False # Reset flags\n        chat_state['image_changed'] = False\n    elif not current_messages: # Allow initial empty message\n        pass\n    else: # No new input, code, or image change\n        print(\"No new input, code, or image change.\")\n        ai_response_content: str = \"Please provide input, type code, or draw on the whiteboard.\"\n        return (\n            gr.update(value=ai_response_content),\n            gr.update(value=current_problem),\n            gr.update(value=current_code),\n            chat_state,\n            gr.update(visible=False),\n            gr.update(),\n        )\n\n    graph_input: Dict[str, Any] = {\"messages\": current_messages, \"question\": current_problem, \"code\": current_code} # , \"image\": current_image}\n    \n    try:\n        new_state: InterviewState = interviewer_graph.invoke(graph_input)\n    except Exception as e:\n        print(f\"Error invoking interviewer graph: {e}\")\n        ai_response_content = f\"An error occurred: {e}\"\n        return (\n            gr.update(value=ai_response_content), \n            gr.update(value=current_problem),     \n            gr.update(value=current_code),        \n            chat_state,                           \n            gr.update(visible=False),\n            gr.update(),\n        )\n\n    ai_response_content: str = \"\"\n    interview_finished_this_turn: bool = new_state.get('finished', False)\n    if new_state and 'messages' in new_state and new_state['messages']:\n        # Find the last AI message to display\n        for msg in reversed(new_state['messages']):\n            if isinstance(msg, AIMessage):\n                # Sanitizing ai message (in some cases model is returning answer in list)\n                ai_response_content = msg.content if type(msg.content) == str else msg.content[0]\n                break\n            elif isinstance(msg, ToolMessage):\n                if msg.name == \"end_interview\" and interview_finished_this_turn:\n                    ai_response_content = \"Thank you for your time. Interview is finished. Reload to start a new one.\"\n                    break\n        if not ai_response_content:\n            ai_response_content = \"Tool executed. Waiting for next step.\"\n\n    else:\n        ai_response_content = \"Error: No messages found in the new state.\"\n\n\n    if interview_finished_this_turn:\n        ai_response_content += \"\\n\\n**Interview finished! Your report is ready for download below.**\" # Append the notice\n        return (\n            gr.update(value=ai_response_content),\n            gr.update(visible=False),\n            gr.update(visible=False),\n            new_state,\n            gr.update(visible=interview_finished_this_turn),\n            gr.update(visible=False),\n        )\n\n    # Update state variables\n    updated_problem_desc: str = new_state.get('question', current_problem)\n    updated_problem_code: str = new_state.get('code', current_code)\n\n    # Create the final state dictionary to return to Gradio\n    final_chat_state: Dict[str, Any] = new_state.copy()\n    final_chat_state['code_changed'] = False\n    final_chat_state['image_changed'] = False\n\n    print(f\"Interview Finished State: {interview_finished_this_turn}\")\n    print(f\"--- Gradio Interaction End ---\")\n\n    return (\n        gr.update(value=ai_response_content),  \n        gr.update(value=updated_problem_desc),\n        gr.update(value=updated_problem_code),\n        final_chat_state,\n        gr.update(visible=interview_finished_this_turn),\n        gr.update(),\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T20:58:23.347574Z","iopub.execute_input":"2025-04-17T20:58:23.347790Z","iopub.status.idle":"2025-04-17T20:58:23.360121Z","shell.execute_reply.started":"2025-04-17T20:58:23.347776Z","shell.execute_reply":"2025-04-17T20:58:23.359207Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# Function to initialize the chat when Gradio load\ndef init_chat() -> Tuple:\n    START_FROM_END: bool = False # for degbuging purposes. Slould initialize test_state at the buttom of the notebook\n    print(\"\\n--- Gradio App Load ---\")\n    initial_problem_text: str = \"Problem has not been selected yet\"\n    initial_problem_code: str = \"# Your code\"\n    initial_state_dict: Dict[str, Any]\n\n    if START_FROM_END: # for degbuging purposes. Slould initialize test_state at the buttom of the notebook\n        initial_state_dict = {\"messages\": test_state[\"messages\"], \n                          \"question\": test_state[\"question\"], \n                          \"code\": test_state[\"code\"],\n                          \"code_changed\": False,\n                          \"image_changfed\": False,\n                          \"finished\": False\n                         }\n    else:\n        initial_state_dict = {\"messages\": [], \n                              \"question\": initial_problem_text, \n                              \"code_changed\": False,\n                              \"image_changfed\": False,\n                              \"finished\": False\n                             }\n    \n    \n    # Run the graph with empty messages to get the initial welcome message\n    initial_state: InterviewState = interviewer_graph.invoke(initial_state_dict)\n    welcome_message: str = \"Error: Could not get initial message.\"\n    if initial_state and initial_state.get('messages'):\n        welcome_message = initial_state['messages'][-1].content\n\n    problem_desc_on_load: str = initial_state.get('question', initial_problem_text)\n    problem_code_on_load: str = initial_state.get('code', initial_problem_code)\n\n    return (\n        welcome_message, \n        problem_desc_on_load, \n        problem_code_on_load, \n        initial_state if initial_state else {}, \n        gr.update(visible=False)\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T20:58:23.361135Z","iopub.execute_input":"2025-04-17T20:58:23.361447Z","iopub.status.idle":"2025-04-17T20:58:23.385068Z","shell.execute_reply.started":"2025-04-17T20:58:23.361400Z","shell.execute_reply":"2025-04-17T20:58:23.384118Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"def mark_code_as_changed(current_state: Dict[str, Any], input_code: str) -> Dict:\n    \"\"\"Update code and code_changes in state when code changes\"\"\"\n    if not current_state:\n        current_state = {}\n    current_state['code_changed'] = True\n    current_state['code'] = input_code\n    return current_state\n\n\ndef mark_image_as_changed(current_state: Dict[str, Any], im: Dict[str, Any]) -> Dict:\n    \"\"\"Update image and image_changed in state when image changes\"\"\"\n    if not current_state:\n        current_state = {}\n    current_state['image_changed'] = True\n    current_state['image'] = im[\"composite\"]\n    return current_state","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T20:58:23.386311Z","iopub.execute_input":"2025-04-17T20:58:23.387006Z","iopub.status.idle":"2025-04-17T20:58:23.406374Z","shell.execute_reply.started":"2025-04-17T20:58:23.386976Z","shell.execute_reply":"2025-04-17T20:58:23.405442Z"}},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"### 6.2. UI Components 🎨🧱\nThe interface consists of:\n*   **Problem Display (`gr.Markdown`):** Shows the description of the currently selected coding problem.\n*   **Interviewer Response (`gr.Markdown`):** Displays the AI interviewer's messages.\n*   **User Input (`gr.Textbox`):** A multi-line text box for the candidate to type their responses.\n*   **Code Input (`gr.Code`):** A code editor with Python syntax highlighting where the candidate can write and submit their code solution.\n*   **Whiteboard (`gr.Sketchpad`):** Allows the candidate to draw diagrams or pseudocode to explain their thinking visually.\n*   **Send Button (`gr.Button`):** Submits the user's text, code (if changed), and whiteboard sketch (if changed) to the backend.\n*   **Download Report Button (`gr.DownloadButton`):** Appears only after the interview is finished, allowing the user to download the generated Markdown evaluation report (`candidate_report.md`).","metadata":{}},{"cell_type":"code","source":"with gr.Blocks() as demo:\n    gr.Markdown(\"## Mock Technologies Inc. - Technical Interviewer\")\n    # Store LangGraph state here\n    chat_state = gr.State({}) # Initialize as an empty dictionary\n\n    problem_display = gr.Markdown(\"Problem has not been selected yet\", container=True, label=\"Selected Problem\")\n    output_text = gr.Markdown(label=\"Interviewer Response\", container=True) # For AI messages\n\n    with gr.Row():\n        with gr.Column(scale=5):\n            input_text = gr.Textbox(label=\"Your message\", placeholder=\"Type your message here...\", lines=5)\n            \n        with gr.Column(scale=5):\n            input_code = gr.Code(\"# Your code\", label=\"Code\", language=\"python\", interactive=True, lines=7)\n            input_code.input(mark_code_as_changed, inputs=[chat_state, input_code], outputs=[chat_state], show_progress=\"hidden\")\n            \n        with gr.Column(scale=1):\n            submit_button = gr.Button(\"Send Message\")\n\n    im = gr.Sketchpad(\n        label='Whiteboard',\n        type=\"pil\",\n        width=600,\n        height=600,\n        canvas_size=(600, 600),\n        layers=gr.LayerOptions(disabled=True),\n    ) \n\n    im.input(mark_image_as_changed, inputs=[chat_state, im], outputs=[chat_state], show_progress=\"hidden\")\n\n    # Download Button - initially hidden\n    download_report = gr.DownloadButton(\n        label=\"Download Interview Report\",\n        value=\"candidate_report.md\", \n        visible=False,\n    )\n\n    # Define the list of components to update on submit button click\n    submit_outputs = [\n        output_text,\n        problem_display,\n        input_code,\n        chat_state,\n        download_report,\n        im\n    ]\n\n    # Define outputs for demo.load (must match init_chat return)\n    load_outputs = [\n        output_text,\n        problem_display,\n        input_code,\n        chat_state,\n        download_report\n    ]\n\n    # Load initial message\n    demo.load(init_chat, inputs=None, outputs=load_outputs)\n\n    submit_button.click(\n        chat_interface_update,\n        inputs=[input_text, chat_state],\n        outputs=submit_outputs\n    )\n    # Clear input textbox after submit\n    submit_button.click(lambda: \"\", outputs=[input_text])","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-04-17T21:12:38.972027Z","iopub.execute_input":"2025-04-17T21:12:38.972366Z","iopub.status.idle":"2025-04-17T21:12:39.174139Z","shell.execute_reply.started":"2025-04-17T21:12:38.972342Z","shell.execute_reply":"2025-04-17T21:12:39.172736Z"}},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":"### ❗ **IMPORTANT!**\n\nThe app built in this notebook uses Gradio launch function to run an UI interface. This section is commented-out to ensure that you can use the `Run all` feature without interruption. At the end of this notebook you will need to uncomment the `demo.launch(...)` call in order to interact with the app.\n\nIf you wish to save a version of this notebook with `Save and Run all`, you will need to **re-comment** the line you commented out to ensure that the notebook can run without human input.","metadata":{}},{"cell_type":"code","source":"# demo.launch(share=True) # Added share=True for Kaggle","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Uncomment and run this to close the gradio session.","metadata":{}},{"cell_type":"code","source":"# demo.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T21:16:30.010044Z","iopub.execute_input":"2025-04-17T21:16:30.011943Z","iopub.status.idle":"2025-04-17T21:16:30.213518Z","shell.execute_reply.started":"2025-04-17T21:16:30.011894Z","shell.execute_reply":"2025-04-17T21:16:30.212188Z"}},"outputs":[{"name":"stdout","text":"Closing server running on port: 7860\n","output_type":"stream"}],"execution_count":39},{"cell_type":"markdown","source":"## 🖼️ 7. Examples\n\n","metadata":{}},{"cell_type":"code","source":"from IPython.display import Image, display\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 🏁 8. Conclusion & Future Work\n\n### Conclusion \nThis project successfully demonstrates the creation of a sophisticated, AI-powered Mock Technical Interviewer using Google Gemini and LangGraph. By integrating features like stateful agentic behavior, function calling, structured output, multimodal input (text, code, image), and grounded recommendations, it provides a valuable and interactive tool for developers preparing for technical interviews. The detailed evaluation report and personalized learning plan offer actionable feedback to help users identify strengths and weaknesses.\n\n### Potential Future Work\n*   **More Question Types:** Expand the `data.json` to include system design, behavioral questions, or questions for different roles/levels.\n*   **Voice Interaction:** Integrate speech-to-text and text-to-speech for a more natural conversational experience.\n*   **Code Execution & Testing:** Allow the agent to actually run the candidate's code against test cases and provide feedback on correctness and performance.\n*   **More Sophisticated Evaluation:** Fine-tune the evaluation prompt or use multiple LLM calls for deeper analysis of specific aspects (e.g., code quality metrics, complexity analysis).\n*   **Persistence:** Allow users to save and resume interview sessions.\n*   **Different Interviewer Personas:** Offer options for different interviewer styles (e.g., more challenging, more collaborative).\n\n    ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}